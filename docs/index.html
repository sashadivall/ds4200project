<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="style.css" />
  <title>Which LLM is the Best At Writing Code? - DS4200 Project</title>
</head>
<body>
  <header>
    <h1>Which LLM is the Best At Writing Code?</h1>

    <p>DS4200 Spring 2025 Final Project</p>
    <p>by Sasha DiVall, Molly Varrenti, and Ben Weiss</p>
  </header>

  <nav>
    <a href="index.html">Home</a>
    <a href="dataset_exploration.html">Interactive Dataset Exploration</a>
    <a href="contact.html">Contact Us</a>
  </nav>


  <main>
    <h2>Introduction</h2>
    <p>
        Using large language models (LLMs) such as ChatGPT to assist in writing code has become a common practice for students and industry
        professionals alike. While LLMs are a great tool for assisting developers in small coding tasks, they often struggle with writing 
        efficient, sensible code. This project investigates the ability of different LLMs to write Python solutions to easy and medium programming
        problems.
    </p>
    <br>
    <h4>Dataset Creation</h4>
    <p class="indent-block"> 
      We created our dataset from scratch by providing a series of prompts to 3 different LLMs. First, we asked ChatGPT to create a list 
      of 50 easy Python programming questions and 50 medium Python programming questions, where easy and medium measure the difficulty of the
      problem. 
      <br>
      The prompt we used: <b><i>Generate a list of 50 easy python programming questions and 50 medium.</i></b>
      <br>
      After rephrasing the prompt to ensure that the problem description was detailed enough, ChatGPT successfully generated a list of 
      50 easy and 50 medium Python questions.
      <br>
      The next step was deciding which models we wanted to evaluate. A group member subscribes to ChatGPT+ which gave us access to a lengthy list 
      of models created by OpenAI. We decided to test the following two OpenAI models: 
      <br>
          <p class="indent-block"><a href="https://openai.com/index/hello-gpt-4o/"><b>gpt-4o</b></a>: Described by OpenAI as "Great for most tasks." Released May 2024.</p>
          <p class="indent-block"><a href="https://openai.com/index/openai-o3-mini/"><b>o3-mini</b></a>: Described by OpenAI as "Fast at advanced reasoning." Released January 2025.</p>
          We wanted to see if a more "advanced" model like o3-mini produced better, faster solutions to the same set of questions than gpt-4o. gpt-4o and o3-mini are only available 
      to those that upgrade to ChatGPT+, so we wanted to choose a third, free model for comparison. We decided on <a href="https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/">Microsoft Copilot</a>.
      <br>
      Once we decided on our models, we asked each of them to create code solutions for each of the 50 easy problems and 50 medium problems and recorded the result to a .csv file.
      For each code snippet, we calculated the number of lines, the number of loops, and the number of parameters. Additionally, we generated a sample input along with expected output to test the 
      accuracy of the code snippet. To determine the actual output and execution time, we ran the code snippet, using the sample input, and measured the time (in ms) it took
      to execute and the output it returned. All code snippets returned an output that was equal to the expected output. Explore the dataset <a href="dataset_exploration.html">here!</a>

    </p>
    <br>
    <br>

    <h2>Results</h2>
    <container class="viz">
      <iframe src="viz1.html" width="1000" height="500" frameborder="0"></iframe>
      <p> <b>Figure 1: </b> In figure one you will see two bar charts comparing the performance of three models (copilot, gpt-4o, and o3-mini) on tasks of varying difficulty (Easy and Medium). The left chart shows the mean execution time in milliseconds for each model, while the right chart displays the mean number of lines of code required.
        The charts indicate that while copilot tends to have higher execution times and longer code lengths compared to gpt-4o and o3-mini across both task difficulties (Easy and Medium), o3-mini consistently performs with lower execution times and shorter code lengths.
      </p>
    </container>
    <container>

      <iframe src="viz2.html" width="600" height="450" frameborder="0"></iframe>
      <br>
      <p> <b>Figure 2: </b> Figure 2 shows a scatter plot titled "log(Execution Time) vs Number of Lines," with the x-axis ranging from 0 to 35 and the y-axis from -5.0 to 1.0. Data points for gpt-4o (blue), o3-mini (green), and Microsoft Copilot (red) are shown, with a legend at the bottom identifying each model.
        The scatter plot highlights the differences in performance characteristics between the three models, with gpt-4o and o3-mini showing more efficient execution times relative to the number of lines of code compared to Microsoft Copilot.
      </p>

    </container>
    <br>
    <container>

      <iframe src="boxplot.html" width="600" height="500" frameborder="0"></iframe>
      <br>
      <p> <b>Figure 3: </b>Figure 3, "Execution Times by Difficulty Level and Model" shows execution times (0 to 0.8 ms) for gpt-4o, o3-mini, and Copilot across Easy and Medium difficulty levels.
        The box plot chart highlights the differences in execution time distributions between the three models, with gpt-4o and o3-mini generally showing faster and more consistent performance compared to copilot.
      </p>

    </container>

    <br>
    <br>

    <container>

        <iframe src="viz3.html" width="900" height="500" frameborder="0"></iframe>
        <br>
        <p> <b>Figure 4: </b> Figure 4's heatmap illustrates the execution time (in milliseconds) versus the number of loops for different models: gpt-4o, o3-mini, and copilot. The color intensity represents the execution time, with darker shades indicating longer times. The heatmap compares execution times across loop counts (0 to 4) for three models: gpt-4o, o3-mini, and copilot. The color gradient shows execution time (0.00 to 0.70 ms).</p>
  
      </container>


    <h2>Conclusions</h2>
    <br>
    <p><b>Performance Consistency:</b> gpt-4o and o3-mini show more efficient and reliable performance compared to Microsoft Copilot, which has higher variability.</p>
    <p><b>Execution Time Trends:</b> Execution time increases with code length, but GPT-4o and o3-mini maintain lower times than Copilot.</p>  
    <p><b>Code Length Efficiency:</b> GPT-4o and o3-mini generate shorter code solutions on average compared to Copilot.</p>
    <p><b>Outliers and Variability:</b> Copilot has more pronounced outliers and variability in execution times, especially for Easy tasks.</p>
    <p><b>Heatmap Insights:</b> he heatmap shows that as the number of loops increases, execution times generally increase for all models, with Copilot exhibiting the highest execution times and variability.</p>
    <p><b>In summary,</b>These conclusions highlight the consistent efficiency of GPT-4o and o3-mini across various metrics, while Copilot shows greater variability and higher execution times, particularly as task complexity increases.</p>
    
  </main>

  <footer>

    <p>DS4200 Final Project Spring 2025 - Which LLM is the Best At Writing Code?</p>
  </footer>
</body>
</html>